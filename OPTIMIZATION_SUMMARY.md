# K·∫ø ho·∫°ch T·ªëi ∆∞u PhoWhisper-Tool - Ho√†n Th√†nh ‚úÖ

## T·ªïng quan

ƒê√£ th·ª±c hi·ªán **12/13 t·ªëi ∆∞u h√≥a l·ªõn** nh·∫±m tƒÉng t·ªëc ƒë·ªô to√†n di·ªán cho PhoWhisper-Tool. D·ª± ki·∫øn **tƒÉng t·ªëc 30-60% t·ªïng th·ªÉ** v·ªõi RTX 3050 Ti 4GB VRAM laptop.

---

## ‚úÖ Optimizations Ho√†n Th√†nh

### **Phase 1: Quick Wins (S·ª≠a l·ªói g√¢y ch·∫≠m)**

#### 1. ‚úÖ X√≥a `enable_model_cpu_offload()` - CRITICAL FIX
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L2631-L2660)
- **V·∫•n ƒë·ªÅ**: H√†m n√†y ƒë·∫©y model **v·ªÅ CPU** khi inference ‚Üí ho√†n to√†n ph·∫£n t√°c d·ª•ng v·ªõi GPU inference
- **Gi·∫£i ph√°p**: X√≥a ho√†n to√†n, ch·ªâ gi·ªØ `.to(device)` v√† `torch.float16`
- **Impact**: **15-30% tƒÉng t·ªëc inference** (r·∫•t l·ªõn!)

#### 2. ‚úÖ Gi·∫£m `torch.cuda.empty_cache()` frequency
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L1330-L1365)
- **Thay ƒë·ªïi**: T·ª´ m·ªói chunk ‚Üí m·ªói 10 chunks + cu·ªëi c√πng
- **Impact**: Gi·∫£m overhead ph√¢n b·ªï memory li√™n t·ª•c, ~2-5% nhanh h∆°n

#### 3. ‚úÖ T√¥n tr·ªçng `--asr-model` flag khi kh·ªüi t·∫°o
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L2945-L2965)
- **V·∫•n ƒë·ªÅ**: Lu√¥n load `PhoWhisper-medium` r·ªìi m·ªõi ki·ªÉm tra flag ‚Üí load model 2 l·∫ßn
- **Gi·∫£i ph√°p**: Ki·ªÉm tra flag tr∆∞·ªõc khi load model
- **Impact**: Ti·∫øt ki·ªám 10-20s th·ªùi gian kh·ªüi ƒë·ªông n·∫øu d√πng `--asr-model whisper`

---

### **Phase 2: Transcription Inference (T√°c ƒë·ªông l·ªõn nh·∫•t)**

#### 4. ‚úÖ Lo·∫°i b·ªè temp file I/O cho chunks
- **V·ªã tr√≠**: 
  - [phowhisper.py](phowhisper.py#L1115-L1260) - `split_audio_to_chunks()` m·ªõi
  - [phowhisper.py](phowhisper.py#L1262-L1300) - `process_chunk()` nh·∫≠n AudioSegment
  - [phowhisper.py](phowhisper.py#L1330-L1365) - Loop s·ª≠ d·ª•ng AudioSegment chunks
- **Thay ƒë·ªïi**: 
  - T·∫°o `split_audio_to_chunks()` tr·∫£ v·ªÅ `List[AudioSegment]` thay v√¨ file paths
  - `process_chunk()` accept c·∫£ AudioSegment v√† file path
  - Convert AudioSegment ‚Üí numpy array ‚Üí HuggingFace pipeline (zero temp file I/O)
- **Impact**: **10-20% nhanh h∆°n** - lo·∫°i b·ªè disk write/read cho ~60 chunks (file 30 ph√∫t)

#### 5. ‚úÖ Th√™m `torch.compile()` cho Whisper model
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L2637-L2670)
- **Thay ƒë·ªïi**: Th√™m `torch.compile(model, mode="reduce-overhead")` v·ªõi try/except fallback
- **Impact**: **15-30% tƒÉng t·ªëc inference** tr√™n RTX 3050 Ti (Ampere architecture)
- **Note**: PyTorch 2.0+ required

#### 6. ‚úÖ T·ªëi ∆∞u language detection
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L355-L420)
- **Thay ƒë·ªïi**: Cleanup `whisper-base` model sau khi detect xong ng√¥n ng·ªØ
- **Impact**: Gi·∫£i ph√≥ng ~300MB VRAM cho Whisper-medium, quan tr·ªçng v·ªõi 4GB VRAM

#### 7. ‚úÖ T·ªëi ∆∞u `find_optimal_audio_speed()`
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L2621-L2705)
- **Thay ƒë·ªïi**: G·ªçi `process_chunk()` tr·ª±c ti·∫øp thay v√¨ `transcribe_audio()` (full pipeline)
- **Impact**: **50-70% nhanh h∆°n** cho speed testing (b·ªè qua split, normalize, analysis)

---

### **Phase 3: Audio Processing**

#### 8. ‚úÖ Vectorize noise reduction v·ªõi `scipy.signal.stft`
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L641-L700)
- **Thay ƒë·ªïi**: 
  - Thay Python `for` loop (~28,000 iterations cho 30 ph√∫t audio)
  - B·∫±ng `scipy.signal.stft()` + `istft()` vectorized
- **Impact**: **5-10x nhanh h∆°n** khi d√πng noise reduction

---

### **Phase 4: Post-processing (LLM)**

#### 9. ‚úÖ GUI s·ª≠ d·ª•ng Ollama pipeline
- **V·ªã tr√≠**: 
  - [phowhisper.py](phowhisper.py#L2278-L2295) - `process_transcript_with_llm()` m·ªõi
  - [gui.py](gui.py#L7-L15) - Import `process_transcript_with_llm`
  - [gui.py](gui.py#L261, L671) - S·ª≠ d·ª•ng unified function
- **Thay ƒë·ªïi**: 
  - T·∫°o unified function `process_transcript_with_llm()`: try Ollama ‚Üí fallback Gemini
  - GUI v√† CLI ƒë·ªÅu d√πng c√πng logic
- **Impact**: 
  - GUI nhanh h∆°n khi Ollama available (local, no API limit)
  - Consistent behavior CLI/GUI

---

### **Phase 5: Memory Management (Quan tr·ªçng v·ªõi 4GB VRAM)**

#### 10. ‚úÖ Unload language detection model sau detect
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L247-L260) cleanup ‚Üí [phowhisper.py](phowhisper.py#L407-L410) g·ªçi sau load_transcriber
- **Impact**: Gi·∫£i ph√≥ng ~300MB VRAM ƒë·ªÉ tr√°nh OOM

#### 11. ‚úÖ Limit CUDA memory fraction (0.9)
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L2637)
- **Thay ƒë·ªïi**: `torch.cuda.set_per_process_memory_fraction(0.9)`
- **Impact**: D√†nh 10% buffer cho OS/display, tr√°nh crash

---

### **Phase 6: Miscellaneous**

#### 12. ‚úÖ Th√™m progress metrics (chunks/sec, RTF)
- **V·ªã tr√≠**: [phowhisper.py](phowhisper.py#L1330-L1365)
- **Thay ƒë·ªïi**: 
  - Track timing per chunk
  - Display chunks/sec m·ªói 5 chunks
  - Display overall Realtime Factor (RTF) khi ho√†n th√†nh
- **Impact**: User visibility - bi·∫øt pipeline ƒëang ch·∫°y hi·ªáu qu·∫£ c·ª° n√†o
- **RTF explained**: RTF < 1.0 = faster than realtime (VD: RTF 0.5 = x·ª≠ l√Ω 2x nhanh h∆°n audio duration)

---

## ‚è∏Ô∏è Optimization Ch∆∞a Th·ª±c Hi·ªán (C√≥ th·ªÉ l√†m sau)

#### 13. ‚ö†Ô∏è Single-pass audio analysis
- **M√¥ t·∫£**: G·ªôp `analyze_audio_characteristics()`, `analyze_background_noise()`, `check_and_adjust_volume()` th√†nh 1 pass
- **Impact**: ~5-10% nhanh h∆°n audio preprocessing
- **L√Ω do skip**: Refactor ph·ª©c t·∫°p, impact kh√¥ng l·ªõn b·∫±ng c√°c optimizations ƒë√£ l√†m

---

## üìä T·ªïng K·∫øt Impact

| Optimization | Impact ∆∞·ªõc t√≠nh | Priority |
|---|---|---|
| 1. X√≥a `enable_model_cpu_offload()` | **15-30%** | üî• CRITICAL |
| 4. Lo·∫°i b·ªè temp file I/O | **10-20%** | üî• HIGH |
| 5. `torch.compile()` | **15-30%** | üî• HIGH |
| 8. Vectorize noise reduction | **5-10x** (khi d√πng) | üî• HIGH |
| 7. Optimize speed testing | **50-70%** (cho test) | üü° MEDIUM |
| 6. Cleanup language detector | 300MB VRAM | üü° MEDIUM |
| 2. Gi·∫£m `empty_cache()` freq | **2-5%** | üü¢ LOW |
| 3. Respect `--asr-model` flag | 10-20s startup | üü¢ LOW |
| 9. GUI Ollama pipeline | Depend on Ollama | üü¢ LOW |
| 12. Progress metrics | UX only | üü¢ LOW |

**D·ª± ki·∫øn t·ªïng th·ªÉ**: **30-60% tƒÉng t·ªëc** cho to√†n b·ªô pipeline (t√πy workload)

---

## üß™ Testing & Verification

### Checklist tr∆∞·ªõc khi release:

- [x] No syntax errors (`get_errors` passed)
- [ ] Test v·ªõi file audio 5 ph√∫t
- [ ] Test v·ªõi file audio 30 ph√∫t (check memory kh√¥ng OOM)
- [ ] Ki·ªÉm tra VRAM usage b·∫±ng `nvidia-smi` (ph·∫£i < 3.8GB peak)
- [ ] So s√°nh output text tr∆∞·ªõc/sau (quality regression test)
- [ ] Test YouTube URL
- [ ] Test Google Drive URL
- [ ] Test c·∫£ 2 model: `--asr-model phowhisper` v√† `--asr-model whisper`
- [ ] Test noise reduction `--noise-reduction`
- [ ] Test GUI
- [ ] Benchmark timing: `time python phowhisper.py --device cuda audio/test.wav`

### Benchmark command:
```bash
# Tr∆∞·ªõc optimization
time python phowhisper.py --device cuda audio/test_30min.wav

# Sau optimization (expected 30-60% faster)
time python phowhisper.py --device cuda audio/test_30min.wav
```

---

## üîß Technical Details

### Thay ƒë·ªïi ch√≠nh trong codebase:

**phowhisper.py**:
- `optimize_model_for_inference()`: X√≥a CPU offload, th√™m `torch.compile()`, th√™m memory fraction limit
- `split_audio_to_chunks()`: NEW - return AudioSegment chunks
- `process_chunk()`: Accept both file path v√† AudioSegment, convert to numpy
- `transcribe_audio()`: D√πng `split_audio_to_chunks()`, track performance metrics
- `_spectral_subtract_channel()`: Vectorized v·ªõi `scipy.signal.stft/istft`
- `find_optimal_audio_speed()`: D√πng `process_chunk()` thay v√¨ `transcribe_audio()`
- `load_transcriber_for_language()`: G·ªçi `cleanup_language_detector()` sau load
- `process_transcript_with_llm()`: NEW - unified Ollama ‚Üí Gemini fallback
- `__main__`: Respect `--asr-model` flag khi kh·ªüi t·∫°o transcriber

**gui.py**:
- Import `process_transcript_with_llm` thay v√¨ `process_transcript_with_gemini`
- S·ª≠ d·ª•ng unified function cho consistent behavior

---

## üí° Recommendations cho User

### ƒê·ªÉ ƒë·∫°t t·ªëc ƒë·ªô t·ªëi ƒëa:

1. **D√πng GPU**: `--device cuda` (m·∫∑c ƒë·ªãnh n·∫øu c√≥ CUDA)
2. **Skip speed test** n·∫øu kh√¥ng c·∫ßn: `--skip-speed`
3. **Ch·ªçn ƒë√∫ng model**: 
   - N·∫øu ch·∫Øc ch·∫Øn Vietnamese: `--asr-model phowhisper`
   - N·∫øu ch·∫Øc ch·∫Øn English/Other: `--asr-model whisper`
   - Auto detect (ch·∫≠m h∆°n ~5s): `--asr-model auto` (default)
4. **Noise reduction**: Ch·ªâ d√πng khi th·∫≠t c·∫ßn thi·∫øt (gi·ªù nhanh h∆°n nh∆∞ng v·∫´n overhead)

### Example optimal command:
```bash
# Vietnamese lecture, skip speed test
python phowhisper.py --asr-model phowhisper --skip-speed audio/lecture.wav

# Auto detect language, keep speed optimization
python phowhisper.py audio/lecture.wav
```

---

## üéØ Next Steps (T√πy ch·ªçn)

N·∫øu mu·ªën optimize th√™m:

1. **Batch inference**: X·ª≠ l√Ω 2-3 chunks song song (c·∫ßn test memory v·ªõi 4GB VRAM)
2. **Flash Attention**: Th√™m `torch.backends.cuda.enable_flash_sdp()` (PyTorch 2.0+)
3. **INT8 quantization**: Gi·∫£m model size, tƒÉng t·ªëc ~1.5-2x (nh∆∞ng gi·∫£m accuracy nh·∫π)
4. **Single-pass audio analysis**: Implement optimization #13
5. **Async LLM processing**: Concurrent requests cho Ollama/Gemini

---

**Generated**: $(date)  
**Optimized for**: RTX 3050 Ti 4GB VRAM Laptop  
**Estimated Speedup**: **30-60%** overall  
**Status**: ‚úÖ **12/13 optimizations completed**
